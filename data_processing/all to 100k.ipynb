{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made by Gibeom LEE, HI LAB\n",
    "# 1M to 100k.ipynb 와 달리, folder_path안에 있는 모든 csv파일을 전부 축소 시켜줘서, 다른 폴더에 저장해주는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from loky import ProcessPoolExecutor\n",
    "from concurrent.futures import as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= 사용자 설정 =================\n",
    "folder_path = r\"C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\conventional 2차\"\n",
    "\n",
    "height        = 0.1          # 피크 감지 임계값 (노이즈 레벨 고려)\n",
    "distance      = 150          # 피크 간 최소 거리(샘플)\n",
    "num_samples   = 100000    # 최종 행 수 (정확히 맞춤)\n",
    "time_mode     = \"similar\"    # \"similar-대충 plot용\" | \"exact-정확한 DFT용\"\n",
    "MAX_WORKERS   = max(1, (os.cpu_count() or 4) - 1)  # 병렬 프로세스 수\n",
    "SEED          = 1234         # 재현성용 시드\n",
    "# ================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 6개 파일 병렬 처리 시작 (mode=similar, workers=27 - LOKY 사용)...\n",
      "\n",
      "ConvDEG_3-6.csv → 총 피크(합집합): 27,567 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-6.csv\n",
      "ConvDEG_3-3.csv → 총 피크(합집합): 27,136 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-3.csv\n",
      "ConvDEG_3-5.csv → 총 피크(합집합): 27,553 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-5.csv\n",
      "ConvDEG_3-2.csv → 총 피크(합집합): 26,966 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-2.csv\n",
      "ConvDEG_3-1.csv → 총 피크(합집합): 25,598 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-1.csv\n",
      "ConvDEG_3-4.csv → 총 피크(합집합): 27,357 | similar / 100,000행 → C:\\Users\\user\\Desktop\\Drive파일\\HI Lab\\0. Projects\\0. On going\\2. Aloe inspired DEG\\0. 실험자료\\4. Data measurement\\251019_데모 데이터\\minimized_conventional 2차\\ConvDEG_3-4.csv\n"
     ]
    }
   ],
   "source": [
    "# 출력 폴더\n",
    "parent_dir = os.path.dirname(folder_path)\n",
    "original_folder_name = os.path.basename(folder_path)\n",
    "out_dir = os.path.join(parent_dir, f\"minimized_{original_folder_name}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---------- 공통 유틸 ---------\n",
    "def read_csv_loose(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path, header=None, skiprows=2, engine=\"c\",\n",
    "            sep=\",\", on_bad_lines=\"error\", encoding=\"utf-8-sig\"\n",
    "        )\n",
    "    except Exception:\n",
    "        df = pd.read_csv(\n",
    "            path, header=None, skiprows=2, engine=\"python\",\n",
    "            sep=r\"[,\\t;]+\", on_bad_lines=\"skip\", encoding=\"utf-8-sig\"\n",
    "        )\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"유효한 채널(열)이 부족합니다. (최소 2열 필요: Time + 1채널)\")\n",
    "    df.columns = [\"Time\"] + [f\"Ch{i}\" for i in range(1, df.shape[1])]\n",
    "    return df\n",
    "\n",
    "def union_peaks(df: pd.DataFrame, height: float, distance: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    모든 채널(Ch1..ChN)의 ±피크 인덱스 합집합.\n",
    "    \"\"\"\n",
    "    idx_list = []\n",
    "    for ch in df.columns[1:]:\n",
    "        y = df[ch].to_numpy(dtype=float, copy=False)\n",
    "        p_pos, _ = find_peaks(y, height=height, distance=distance)\n",
    "        p_neg, _ = find_peaks(-y, height=height, distance=distance)\n",
    "        if p_pos.size: idx_list.append(p_pos)\n",
    "        if p_neg.size: idx_list.append(p_neg)\n",
    "    if not idx_list:\n",
    "        return np.array([], dtype=int)\n",
    "    # 중복 제거 + 정렬 1회\n",
    "    return np.unique(np.concatenate(idx_list).astype(int, copy=False))\n",
    "\n",
    "def select_indices(total_len: int, peaks: np.ndarray, num_samples: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    피크 우선 + 균등 샘플로 인덱스 선택, 중복 제거, 최종 행수 강제 보정.\n",
    "    \"\"\"\n",
    "    if peaks.size >= num_samples:\n",
    "        return np.sort(rng.choice(peaks, num_samples, replace=False))\n",
    "    need = num_samples - peaks.size\n",
    "    all_idx = np.arange(total_len, dtype=int)\n",
    "    non_peaks = np.setdiff1d(all_idx, peaks, assume_unique=True)\n",
    "    if need > 0 and non_peaks.size > 0:\n",
    "        # 균등 간격 인덱스 선택(빠르고 예측 가능)\n",
    "        add = non_peaks[np.linspace(0, non_peaks.size - 1, need, dtype=int)]\n",
    "        sel = np.unique(np.concatenate((peaks, add)))\n",
    "    else:\n",
    "        sel = np.unique(peaks)\n",
    "    # 강제 보정\n",
    "    if sel.size > num_samples:\n",
    "        sel = sel[:num_samples]\n",
    "    elif sel.size < num_samples and sel.size > 0:\n",
    "        pad = rng.choice(sel, num_samples - sel.size, replace=True)\n",
    "        sel = np.sort(np.concatenate((sel, pad)))\n",
    "    return sel\n",
    "\n",
    "# ---------- 모드 구현 ----------\n",
    "def mode_similar(df: pd.DataFrame, selected_indices: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    균일 시간 격자(0초 시작)로 재배열.\n",
    "    - 선택된 지점은 최근접 그리드에 실제 샘플값(피크 포함)으로 스냅 → 피크 크기/모양 보존\n",
    "    - 비어 있는 구간은 선형보간으로 채움\n",
    "    \"\"\"\n",
    "    t0 = float(df[\"Time\"].iloc[0])\n",
    "    t1 = float(df[\"Time\"].iloc[-1])\n",
    "    t_orig = df[\"Time\"].to_numpy(dtype=float, copy=False) - t0\n",
    "\n",
    "    sel = np.asarray(selected_indices, dtype=int)\n",
    "    N = sel.size\n",
    "    # 균일 격자: 0 ~ round(T), endpoint=False\n",
    "    T = max(0.0, t1 - t0)\n",
    "    T_round = round(T)\n",
    "    if N <= 1:\n",
    "        t_new = np.zeros(N, dtype=float)\n",
    "    else:\n",
    "        t_new = np.linspace(0.0, T_round, N, endpoint=False)\n",
    "\n",
    "    out = pd.DataFrame({\"Time\": t_new})\n",
    "    # 선형보간으로 채워두기\n",
    "    for ch in df.columns[1:]:\n",
    "        y = df[ch].to_numpy(dtype=float, copy=False)\n",
    "        out[ch] = np.interp(t_new, t_orig, y)\n",
    "\n",
    "    if N == 0:\n",
    "        return out\n",
    "\n",
    "    # 선택 샘플 → 최근접 버킷\n",
    "    t_sel = t_orig[sel]\n",
    "    j = np.searchsorted(t_new, t_sel, side=\"left\")\n",
    "    j = np.clip(j, 0, N - 1)\n",
    "\n",
    "    # 좌/우 버킷 비교로 진짜 최근접 찾기\n",
    "    # (벡터화: 왼/오 인덱스 계산 후, 더 가까운 쪽 선택)\n",
    "    left = np.maximum(j - 1, 0)\n",
    "    right = np.minimum(j + 1, N - 1)\n",
    "    # 거리 계산\n",
    "    dist_j = np.abs(t_sel - t_new[j])\n",
    "    dist_l = np.abs(t_sel - t_new[left])\n",
    "    dist_r = np.abs(t_sel - t_new[right])\n",
    "    # 초기 nearest=j, 더 가까우면 교체\n",
    "    nearest = j.copy()\n",
    "    mask_l = dist_l < np.abs(t_sel - t_new[nearest])\n",
    "    nearest = np.where(mask_l, left, nearest)\n",
    "    mask_r = dist_r < np.abs(t_sel - t_new[nearest])\n",
    "    nearest = np.where(mask_r, right, nearest)\n",
    "\n",
    "    # 충돌 시 절댓값 큰 값 유지\n",
    "    for ch in df.columns[1:]:\n",
    "        y = df[ch].to_numpy(dtype=float, copy=False)\n",
    "        y_sel = y[sel]\n",
    "        # 같은 버킷에 여러 값이 몰릴 수 있으므로, 버킷별 최대 절댓값 선택\n",
    "        # 현재 out[ch]와 비교하여 더 큰 절댓값이면 덮어쓰기\n",
    "        current = out[ch].to_numpy()\n",
    "        # 한 번에 적용: 같은 버킷에 여러 항목이면 마지막 것이 남으므로,\n",
    "        # 버킷 단위로 최대치 인덱스를 구해 적용\n",
    "        # 간단/빠른 방법: 반복(채널 수가 적으므로 충분히 빠름)\n",
    "        for k in range(N):\n",
    "            b = nearest[k]\n",
    "            if abs(y_sel[k]) >= abs(current[b]):\n",
    "                current[b] = y_sel[k]\n",
    "        out[ch] = current\n",
    "\n",
    "    return out\n",
    "\n",
    "def mode_exact(df: pd.DataFrame, selected_indices: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    원래 시간/값 그대로(비균일 간격). 피크 시점/값 절대 변경 없음.\n",
    "    \"\"\"\n",
    "    return df.iloc[selected_indices].copy()\n",
    "\n",
    "def process_one(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    한 파일 처리(개별 프로세스에서 실행). 로그 문자열 반환.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(SEED)  # 각 프로세스 동일 시드 → 결과 재현 가능(원하면 os.getpid()로 섞어도 됨)\n",
    "    try:\n",
    "        df = read_csv_loose(file_path)\n",
    "        peaks = union_peaks(df, height=height, distance=distance)\n",
    "        num_peaks = peaks.size\n",
    "\n",
    "        sel = select_indices(len(df), peaks, num_samples, rng)\n",
    "\n",
    "        if time_mode == \"similar\":\n",
    "            out_df = mode_similar(df, sel)\n",
    "        elif time_mode == \"exact\":\n",
    "            out_df = mode_exact(df, sel)\n",
    "        else:\n",
    "            return f\"[ERR] {os.path.basename(file_path)}: time_mode='{time_mode}'가 올바르지 않습니다.\"\n",
    "\n",
    "        save_path = os.path.join(out_dir, os.path.basename(file_path))\n",
    "        out_df.to_csv(save_path, index=False, header=False)\n",
    "\n",
    "        return (f\"{os.path.basename(file_path)} → 총 피크(합집합): {num_peaks:,d} | \"\n",
    "                f\"{time_mode} / {len(out_df):,d}행 → {save_path}\")\n",
    "    except Exception as e:\n",
    "        return f\"[ERR] {os.path.basename(file_path)}: {e}\"\n",
    "\n",
    "def main():\n",
    "    files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not files:\n",
    "        print(\"CSV 파일이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"총 {len(files)}개 파일 병렬 처리 시작 \"\n",
    "          f\"(mode={time_mode}, workers={MAX_WORKERS} - LOKY 사용)...\\n\")\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # ProcessPoolExecutor를 loky 것으로 사용합니다.\n",
    "    # mp_context 인수를 제거하고 기존과 같이 사용합니다.\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(process_one, p): p for p in files}\n",
    "        for fut in as_completed(futs):\n",
    "            p = futs[fut]\n",
    "            try:\n",
    "                print(fut.result())\n",
    "            except Exception as e:\n",
    "                print(f\"[ERR] {os.path.basename(p)}: {e}\")\n",
    "\n",
    "    # 완료 로그\n",
    "    for line in results:\n",
    "        print(line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
